###train config file
#file path and dir
train_file=data/train_bert_ner_input.txt
dev_file=data/valid_bert_ner_input.txt
#test_file=data/valid_bert_ner_input.txt
test_file=data/test_bert_ner_input.txt
#oov_file=../data/ali_7k/test_oov_pos_bi
word_embed_path=
word_embed_save=data/ccks/train_sp_embed
char_embed_path=
char_embed_save=
bert_dim=768
model_save_dir=snapshot/modelbest.pkl
result_save_dir=result_shot/
model_path=snapshot/modelbest.pkl

#hyperparameters
use_char=False
use_cuda=True
use_crf=False
use_elmo=False
use_bert=True
pretrain=True
word_embed_dim=100
char_embed_dim=30
optimizer=BertAdam
hidden_dim=150
fine_tune=True
elmo_fine_tune=True
char_hidden_dim=150
lstm_layer=1
bilstm=True
cnn_layer=1
dropout=0.5
lr=1e-5
lr_decay=0.05
momentum=0
weight_decay=0
iter=50
batch_size=10
attention=False
lstm_attention=False
attention_dim=300
average_loss=True
norm_word_emb=False
norm_char_emb=False
tag_scheme=BIOES
number_normalized=True
threshold=0
max_sent_len=250
entity_mask=False
mask_percent=0.1
stopwords=
#feature=POS embed_dim=20
hyperlstm=False
hyper_hidden_dim=50
hyper_emb_dim=512

#other_config
dataset=conll2003
status=tag 
#status=train 
word_seq_feature=LSTM
char_seq_feature=LSTM

#bert
bert_path=../data/bert-base-chinese.tar.gz
bert_embedding_size=768
bert_vocab_path=../data/bert-base-chinese-vocab.txt